{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas langchain openai transformers\n",
    "!pip install langchain_community\n",
    "!pip install -q langchainhub\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q sentence_transformers\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain,LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, Splitting and Indexing Data in Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is to be done once. Initially all the data in the preprocessed.csv file will be loaded, splitted and indexed in the Chroma DB stored locally. \n",
    "\n",
    "Next time when we need to reload the vector db, we can easily use the presisiting db and load it from the memory saving computation in restoring the same data again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "def load_documents_from_csv(csv_path):\n",
    "    # Assuming your CSV has a column named \"content\" containing the text\n",
    "    data = pd.read_csv(csv_path)\n",
    "    documents = [Document(content) for content in data['content']]\n",
    "    return documents\n",
    "\n",
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        chunks, embedding=embeddings, persist_directory = CHROMA_PATH\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}\")\n",
    "    return vectordb\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents_from_csv(\"processed_articles.csv\")\n",
    "    chunks = split_text(documents)\n",
    "    vectordb = save_to_chroma(chunks)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = generate_data_store()\n",
    "# retriever = vectordb.as_retriever(search_type = \"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Vector Database stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML-Question\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query):\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        model_kwargs={\"temperature\":0.2, \"max_length\":512},\n",
    "        huggingfacehub_api_token = \"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\"\n",
    "    )\n",
    "\n",
    "    print('In Search Mode')\n",
    "    rqa_prompt_template = \"\"\"Use the following pieces of context to answer the questions at the end.\n",
    "                        Answer only from the context. If you don't know the answer, say you do not know.\n",
    "                    {context}\n",
    "                    Explain in detail.\n",
    "                    Question: {question}\n",
    "                    \"\"\"\n",
    "    RQA_PROMPT = PromptTemplate(\n",
    "        template = rqa_prompt_template, input_variables = [\"context\",\"question\"]\n",
    "    )\n",
    "    rqa_chain_type_kwargs = {\"prompt\": RQA_PROMPT}\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_type = \"similarity\", search_kwargs={\"k\": 5})\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm,\n",
    "                                     chain_type=\"stuff\",\n",
    "                                     retriever = retriever,\n",
    "                                     chain_type_kwargs=rqa_chain_type_kwargs,\n",
    "                                     return_source_documents = True,\n",
    "                                     verbose = False)\n",
    "    result = qa({\"query\": query})\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Ask a question: \")\n",
    "    print(\"looking for results\")\n",
    "    result = main(query)\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "        repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        model_kwargs={\"temperature\":0.2, \"max_length\":2000},\n",
    "        huggingfacehub_api_token = \"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\"\n",
    "    )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_output(output):\n",
    "    output_ind = output.find(\"Answer\")\n",
    "    return output[output_ind:]\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | format_output\n",
    ")\n",
    "\n",
    "ans = rag_chain.invoke(\"What happened at the Al-Shifa Hospital?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: The Al-Shifa Hospital in Gaza was attacked by the Israeli military in November 2020. The hospital was used as a command center by Hamas, according to the military, but the World Health Organization (WHO) described the conditions in the hospital as a bloodbath, with new patients arriving every minute and the injured being sutured with little to no anesthesia. The hospital was once the largest in Gaza but has been closed for weeks.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing the context with More information from online sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def fetch_dbpedia_data(query):\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    sparql.setQuery(f\"\"\"\n",
    "    SELECT ?abstract WHERE {{\n",
    "        ?article dbo:abstract ?abstract .\n",
    "        ?article rdfs:label \"{query}\"@en .\n",
    "        FILTER (lang(?abstract) = 'en')\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"abstract\"][\"value\"]\n",
    "    return \"No relevant information found on DBpedia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No relevant information found on DBpedia.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_dbpedia_data(\"Al-Shifa Hospital?\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Al-Shifa Hospital in Gaza was attacked by Israeli forces in November 2020. The hospital was used as a command center by Hamas, according to the Israeli military, but the World Health Organization (WHO) disputed this claim. The WHO described the conditions in the hospital as a \"bloodbath\" and reported that several patients had fled on foot because ambulances could not reach the facility. The Israeli military detained dozens of suspected militants\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "        repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        model_kwargs={\"temperature\":0.2, \"max_length\":2000},\n",
    "        huggingfacehub_api_token = \"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\"\n",
    "    )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_output(output):\n",
    "    output_ind = output.find(\"Answer\")\n",
    "    return output[output_ind:]\n",
    "\n",
    "\n",
    "def augment_context_with_dbpedia(question):\n",
    "    data = fetch_dbpedia_data(question)\n",
    "    return data\n",
    "\n",
    "rag_chain = (\n",
    "  {\n",
    "    \"context\": RunnableParallel({\"context 1\":retriever | format_docs, \"context 2\": augment_context_with_dbpedia}),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "  }\n",
    "  | prompt\n",
    "  | llm\n",
    "  | format_output\n",
    ")\n",
    "\n",
    "ans = rag_chain.invoke(\"What happened at the Al-Shifa Hospital?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
